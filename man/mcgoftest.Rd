% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mcgoftest.R
\name{mcgoftest}
\alias{mcgoftest}
\title{Bootstrap test for Goodness of fit (GoF)}
\usage{
mcgoftest(
  varobj,
  distr,
  pars,
  num.sampl = 999,
  sample.size,
  stat = c("ks", "ad", "rmse", "chisq"),
  breaks = NULL,
  parametric = TRUE,
  seed = 1,
  num.cores = 1,
  tasks = 0,
  verbose = TRUE
)
}
\arguments{
\item{varobj}{A a vector containing observations, the variable for which the
CDF parameters was estimated.}

\item{distr}{The name of the cummulative distribution function (CDF) or a
concrete CDF from where estimate the cummulative probabilities.
Distribution \emph{distr} must be defined in environment-namespace from
any package or environment defined by user.}

\item{pars}{CDF model parameters. A list of parameters to evaluate the CDF.}

\item{num.sampl}{Number of resamplings.}

\item{sample.size}{Size of the samples used for each sampling.}

\item{stat}{One string denoting the statistic to used in the testing: "ks":
Kolmogorov–Smirnov, "ad": Anderson–Darling statistic, "chisq: Pearson's
Chi-squared, and "rmse": Root Mean Square of the error.}

\item{breaks}{Default is NULL. Basically, the it is same as in function
\code{\link[graphics]{hist}}. If \emph{breaks} = NULL, then function
'nclass.FD' (see \code{\link[grDevices]{nclass}} is applied to estimate
the breaks.}

\item{parametric}{Logical object. If TRUE, then samples are drawn from the
theoretical population described by \emph{distr}. Default: TRUE.}

\item{seed}{An integer used to set a 'seed' for random number generation.}

\item{num.cores, tasks}{Paramaters for parallele computation using package
\code{\link[BiocParallel]{BiocParallel-package}}: the number of cores to
use, i.e. at most how many child processes will be run simultaneously
(see \code{\link[BiocParallel]{bplapply}} and the number of tasks per job
(only for Linux OS).}

\item{verbose}{if verbose, comments and progress bar will be printed.}
}
\value{
A numeric vector with the following data:
    \enumerate{

        \item Statistic value.

        \item mc_p.value: the probability of finding the observed, or more
              extreme, results when the null hypothesis \eqn{H_0} of a study
              question is true obtained Monte Carlo resampling approach.
    }
}
\description{
To accomplish the nonlinear fit of a probability distribution
    function (*PDF*), dIfferent optimization algorithms can be used. Each
    algorithm will return a different set of estimated parameter values. AIC
    and BIC are not useful (in this case) to decide which parameter set of
    values is the best. The goodness-of-fit tests (GOF) can help in this
    case. Please, see below the examples on how to use this function.
}
\details{
The test is intended for continuos distributions. If sampling size
    is lesser the size of the sample, then the test becomes a Monte Carlo
    test. The thes is based on the use of measures of goodness of fit,
    statistics. The following statistics are availible:

    \itemize{
        \item Kolmogorov- Smirnov statistic (ks). Limitations: sensitive to
              ties [1]. Only the parametric Monte Carlo resampling
              (provided that there is not ties in the data) can be used.

        \item Anderson–Darling statistic (ad) [2]. Limitation: by
              construction, it depends on the sample size. So, the size of
              the sampling must be close to the sample size if Monte Carlo
              resampling is used, which could be a limitation if the sample
              size is too large [2]. In particular, could be an issue in some
              genomic applications. It is worth highlighting that, for the
              current application, the Anderson–Darling statistic is not
              standardized as typically done in testing GoF for normal
              distribution with Anderson–Darling test. It is not required
              since, the statistic is not compared with a corresponding
              theoretical value. In addition, since the computation of this
              statistic requires for the data to be put in order [2], it does
              not make sense to perform a permutation test. That is, the
              maximum sampling size is the sample size less 1.

        \item Pearson's Chi-squared statistic (chisq). Limitation: the sample
              must be discretized (partitioned into bins), which is could be
              a source of bias that leads to the rejection of the null
              hypothesis. Here, the discretization is done using function
              the resources from function \code{\link[graphics]{hist}}.

        \item Root Mean Square statistic (rmse). Limitation: the same
              as 'chisq'.
    }
}
\examples{
## ======== Example 1 =======
# Let us generate a random sample a from a specified Weibull distribution:
# Set a seed
set.seed( 1 )
# Random sample from Weibull( x | shape = 0.5, scale = 1.2 )
x = rweibull(10000, shape = 0.5, scale = 1.2)

# MC KS test accept the null hypothesis that variable x comes
# from Weibull(x | shape = 0.5, scale = 1.2), while the standard
# Kolmogorov-Smirnov test reject the Null Hypothesis.
mcgoftest(x, distr = "weibull", pars = c( 0.5, 1.2 ), num.sampl = 500,
        sample.size = 1000, num.cores = 4)

## ========= Example 2 ======
# Let us generate a random sample a random sample from a specified Normal
# distribution:
# Set a seed
set.seed( 1 )
x = rnorm(10000, mean = 1.5, sd = 2)

# MC KS test accept the null hypothesis that variable x comes
# from N(x | mean = 0.5, sd = 1.2), while the standard
# Kolmogorov-Smirnov test reject the Null Hypothesis.
mcgoftest(x, distr = "norm", pars = c(1.5, 2), num.sampl = 500,
          sample.size = 1000, num.cores = 1)

## ========= Example 3 ======
## Define a Weibull 3-parameter distribution function
pwdist <- function(x, pars) pweibull(x - pars[1], shape = pars[2],
                   scale = pars[3])
rwdist <- function(n, pars) rweibull(n, shape = pars[2],
                    scale = pars[3]) + pars[1]

## A random generation from Weibull-3P
set.seed(123)
pars <- c(mu = 0.9, shape = 1.4, scale = 3.7)
w <- rwdist(200, pars = pars)

## Testing GoF
mcgoftest(varobj = w, distr = "wdist", pars = list(pars), num.sampl = 100,
          sample.size =  199, stat = "chisq", num.cores = 4, breaks = 100,
          seed = 123)

## ========= Example 4 ======
## ----- Testing GoF of a mixture distribution. ----
## Define a mixure distribution to be avaluated with functions 'mixtdistr'
## (see ?mixtdistr). In the current case, it will be mixture of a Log-Normal
## and a Weibull distributions:

phi = c( 0.37, 0.63) # Mixture proportions
args <- list(lnorm = c(meanlog = 0.837, sdlog = 0.385),
             weibull = c(shape = 2.7, scale = 5.8))
##  Sampling from the specified mixture distribution
set.seed(123)
x <- rmixtdistr(n = 1e5, phi = phi , arg = args)
hist(x, 100, freq = FALSE)
x1 <- seq(0, 10, by = 0.001)
lines(x1, dmixtdistr(x1, phi = phi, arg = args), col = "red")

## The GoF for the simulated sample
pars <- c(list(phi = phi), arg = list(args))
mcgoftest(varobj = x, distr = "mixtdistr", pars = pars, num.sampl = 999,
        sample.size =  999, stat = "chisq", num.cores = 4, breaks = 200,
        seed = 123)

}
\references{
\enumerate{
        \item Feller, W. On the Kolmogorov-Smirnov Limit Theorems for
            Empirical Distributions. Ann. Math. Stat. 19, 177–189 (1948).
        \item Anderson, T. . & Darling, D. A. A Test Of Goodness Of Fit. J.
            Am. Stat. Assoc. 49, 765–769 (1954).
        \item Watson, G. S. On Chi-Square Goodness-Of-Fit Tests for
            Continuous Distributions. J. R. Stat. Soc. Ser. B Stat.
            Methodol. 20, 44–72 (1958).
    }
}
\seealso{
Distribution fitting: \code{\link{fitMixDist}},
    \code{\link[MASS]{fitdistr}}, \code{\link{fitCDF}}.
}
\author{
Robersy Sanchez (\url{https://genomaths.com}).
}
