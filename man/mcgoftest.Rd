% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mcgoftest.R
\name{mcgoftest}
\alias{mcgoftest}
\title{Bootstrap test for Goodness of fit (GoF)}
\usage{
mcgoftest(
  varobj,
  distr,
  pars = NULL,
  num.sampl = 999,
  sample.size,
  stat = c("ks", "ad", "rmse", "chisq", "hd"),
  breaks = NULL,
  arg.names = NULL,
  parametric = TRUE,
  seed = 1,
  num.cores = 1,
  tasks = 0,
  verbose = TRUE
)
}
\arguments{
\item{varobj}{A a vector containing observations, the variable for which the
CDF parameters was estimated or the discrete absolute frequencies of each
observation category.}

\item{distr}{The possible options are:
\enumerate{
   \item The name of the cumulative distribution function (CDF).
   \item A concrete CDF from where estimate the cumulative probabilities.
}}

\item{pars}{CDF model parameters. A list of parameters to evaluate the CDF.}

\item{num.sampl}{Number of resamplings.}

\item{sample.size}{Size of the samples used for each sampling.}

\item{stat}{One string denoting the statistic to used in the testing:
\enumerate{
   \item "ks": Kolmogorov–Smirnov.
   \item "ad": Anderson–Darling statistic.
   \item "chisq: Pearson's Chi-squared.
   \item "rmse": Root Mean Square of the error.
   \item "hd": Hellinger Divergence statistics.
}}

\item{breaks}{Default is NULL. Basically, the it is same as in function
\code{\link[graphics]{hist}}. If \emph{breaks} = NULL, then function
'nclass.FD' (see \code{\link[grDevices]{nclass}} is applied to estimate
the breaks.}

\item{arg.names}{(Optional) The names of the arguments from function
\strong{\emph{distr}}.}

\item{parametric}{Logical object. If TRUE, then samples are drawn from the
theoretical population described by \emph{distr}. Default: TRUE.}

\item{seed}{An integer used to set a 'seed' for random number generation.}

\item{num.cores, tasks}{Parameters for parallel computation using package
\code{\link[BiocParallel]{BiocParallel-package}}: the number of cores to use,
i.e. at most how many child processes will be run simultaneously (see
\code{\link[BiocParallel]{bplapply}} and the number of tasks per job (only
for Linux OS).}

\item{verbose}{if verbose, comments and progress bar will be printed.}
}
\value{
A numeric vector with the following data:
    \enumerate{

        \item Statistic value.

        \item mc_p.value: the probability of finding the observed, or more
              extreme, results when the null hypothesis \eqn{H_0} of a study
              question is true obtained Monte Carlo resampling approach.
    }
}
\description{
To accomplish the nonlinear fit of a probability distribution
function \emph{(PDF)}, different optimization algorithms can be used. Each
algorithm will return a different set of estimated parameter values. AIC and
BIC are not useful (in this case) to decide which parameter set of values is
the best. The goodness-of-fit tests \emph{(GOF)} can help in this case.
Please, see below the examples on how to use this function.
}
\details{
The test is intended for mostly continuous distributions.
If sampling size is lesser the size of the sample, then the test becomes a
Monte Carlo test. The test is based on the use of measures of goodness of
fit, statistics. The following statistics are available (and some
limitations for their application to continuous variables are given):

    \itemize{
        \item Kolmogorov- Smirnov statistic (ks). Limitations: sensitive to
              ties [1]. Only the parametric Monte Carlo resampling
              (provided that there is not ties in the data) can be used.

        \item Anderson–Darling statistic (ad) [2]. Limitation: by
              construction, it depends on the sample size. So, the size of
              the sampling must be close to the sample size if Monte Carlo
              resampling is used, which could be a limitation if the sample
              size is too large [2]. In particular, could be an issue in some
              genomic applications. It is worth highlighting that, for the
              current application, the Anderson–Darling statistic is not
              standardized as typically done in testing GoF for normal
              distribution with Anderson–Darling test. It is not required
              since, the statistic is not compared with a corresponding
              theoretical value. In addition, since the computation of this
              statistic requires for the data to be put in order [2], it does
              not make sense to perform a permutation test. That is, the
              maximum sampling size is the sample size less 1.

        \item Pearson's Chi-squared statistic (chisq). Limitation: the sample
              must be discretized (partitioned into bins), which is could be
              a source of bias that leads to the rejection of the null
              hypothesis. Here, the discretization is done using function
              the resources from function \code{\link[graphics]{hist}}.

        \item Root Mean Square statistic (rmse). Limitation: the same
              as 'chisq'.
        \item Hellinger Divergence statistic (hd). Limitation: the same
              as 'chisq'.
       }
If the argument \strong{\emph{distr}} must be defined in
environment-namespace from any package or the environment defined by the
user. if  \eqn{missing( sample.size )} or
\eqn{sample.size > length(varobj)}, then
\eqn{sample.size <- length(varobj) - 1}.

Notice that 'chisq', 'rmse', and 'hd' tests can be applied to testing two
discrete probability distributions as well. However, here,
\strong{\emph{mcgoftest}} function is limited to continuous probability
distributions.
}
\examples{
## ======== Example 1 =======
# Let us generate a random sample a from a specified Weibull distribution:
# Set a seed
set.seed( 1 )
# Random sample from Weibull( x | shape = 0.5, scale = 1.2 )
x = rweibull(10000, shape = 0.5, scale = 1.2)

# MC KS test accept the null hypothesis that variable x comes
# from Weibull(x | shape = 0.5, scale = 1.2), while the standard
# Kolmogorov-Smirnov test reject the Null Hypothesis.
mcgoftest(x, distr = "weibull", pars = c( 0.5, 1.2 ), num.sampl = 500,
        sample.size = 1000, num.cores = 4)

## ========= Example 2 ======
# Let us generate a random sample a random sample from a specified Normal
# distribution:
# Set a seed
set.seed( 1 )
x = rnorm(10000, mean = 1.5, sd = 2)

# MC KS test accept the null hypothesis that variable x comes
# from N(x | mean = 0.5, sd = 1.2), while the standard
# Kolmogorov-Smirnov test reject the Null Hypothesis.
mcgoftest(x, distr = "norm", pars = c(1.5, 2), num.sampl = 500,
          sample.size = 1000, num.cores = 1)

## ========= Example 3 ======
## Define a Weibull 3-parameter distribution function
pwdist <- function(x, pars) pweibull(x - pars[1], shape = pars[2],
                   scale = pars[3])
rwdist <- function(n, pars) rweibull(n, shape = pars[2],
                    scale = pars[3]) + pars[1]

## A random generation from Weibull-3P
set.seed(123)
pars <- c(mu = 0.9, shape = 1.4, scale = 3.7)
w <- rwdist(200, pars = pars)

## Testing GoF
mcgoftest(varobj = w, distr = "wdist", pars = list(pars), num.sampl = 100,
          sample.size =  199, stat = "chisq", num.cores = 4, breaks = 100,
          seed = 123)

## ========= Example 4 ======
## ----- Testing GoF of a mixture distribution. ----
## Define a mixure distribution to be avaluated with functions 'mixtdistr'
## (see ?mixtdistr). In the current case, it will be mixture of a Log-Normal
## and a Weibull distributions:

phi = c( 0.37, 0.63) # Mixture proportions
args <- list(lnorm = c(meanlog = 0.837, sdlog = 0.385),
             weibull = c(shape = 2.7, scale = 5.8))
##  Sampling from the specified mixture distribution
set.seed(123)
x <- rmixtdistr(n = 1e5, phi = phi , arg = args)
hist(x, 100, freq = FALSE)
x1 <- seq(0, 10, by = 0.001)
lines(x1, dmixtdistr(x1, phi = phi, arg = args), col = "red")

## The GoF for the simulated sample
pars <- c(list(phi = phi), arg = list(args))
mcgoftest(varobj = x, distr = "mixtdistr", pars = pars, num.sampl = 999,
        sample.size =  999, stat = "chisq", num.cores = 4, breaks = 200,
        seed = 123)


}
\references{
\enumerate{
        \item Feller, W. On the Kolmogorov-Smirnov Limit Theorems for
            Empirical Distributions. Ann. Math. Stat. 19, 177–189 (1948).
        \item Anderson, T. . & Darling, D. A. A Test Of Goodness Of Fit. J.
            Am. Stat. Assoc. 49, 765–769 (1954).
        \item Watson, G. S. On Chi-Square Goodness-Of-Fit Tests for
            Continuous Distributions. J. R. Stat. Soc. Ser. B Stat.
            Methodol. 20, 44–72 (1958).
        \item A. Basu, A. Mandal, L. Pardo, Hypothesis testing for two
            discrete populations based on the Hellinger distance. Stat.
            Probab. Lett. 80, 206–214 (2010).
    }
}
\seealso{
Distribution fitting: \code{\link{fitMixDist}},
    \code{\link[MASS]{fitdistr}}, \code{\link{fitCDF}}.
}
\author{
Robersy Sanchez (\url{https://genomaths.com}).
}
